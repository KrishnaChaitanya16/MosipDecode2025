{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-27T12:49:03.079289Z","iopub.execute_input":"2025-09-27T12:49:03.079508Z","iopub.status.idle":"2025-09-27T12:49:03.083822Z","shell.execute_reply.started":"2025-09-27T12:49:03.079492Z","shell.execute_reply":"2025-09-27T12:49:03.083325Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"import json\nimport re\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\n\nclass QwenFieldMapper:\n    def __init__(self, model_name=\"Qwen/Qwen2.5-1.5B-Instruct\"):\n        print(f\"Loading Qwen model: {model_name}\")\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n\n        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        self.model = AutoModelForCausalLM.from_pretrained(\n            model_name,\n            torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n            device_map=\"auto\" if device == \"cuda\" else None,\n            low_cpu_mem_usage=True,\n            trust_remote_code=True\n        )\n        if device == \"cpu\":\n            self.model = self.model.to(device)\n        self.device = device\n        print(f\"Model loaded on {device}\")\n\n    def extract_fields(self, ocr_text: str, required_fields: list[str]) -> dict:\n        \"\"\"\n        Extract the given required_fields from ocr_text and return only JSON \n        with those exact keys.\n        \"\"\"\n        # Build a JSON skeleton dynamically\n        skeleton = \"{\\n\"\n        skeleton += \",\\n\".join([f'  \"{field}\": \"\"' for field in required_fields])\n        skeleton += \"\\n}\"\n\n        prompt = f\"\"\"Extract information from the following text and return ONLY a JSON object\nwith these exact field names (no other text or keys):\n\nText: {ocr_text}\n\nReturn only this JSON format:\n{skeleton}\n\"\"\"\n\n        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.device)\n        with torch.no_grad():\n            outputs = self.model.generate(\n                **inputs,\n                max_new_tokens=300,\n                temperature=0.05,\n                do_sample=False,\n                pad_token_id=self.tokenizer.pad_token_id,\n                eos_token_id=self.tokenizer.eos_token_id,\n                repetition_penalty=1.1\n            )\n\n        generated_tokens = outputs[0][inputs['input_ids'].shape[-1]:]\n        generated = self.tokenizer.decode(generated_tokens, skip_special_tokens=True)\n\n        return self._extract_strict_json(generated, required_fields)\n\n    def _extract_strict_json(self, text: str, required_fields: list[str]) -> dict:\n        \"\"\"\n        Extracts the first valid JSON object from text and ensures all required fields exist.\n        \"\"\"\n        default_result = {field: \"\" for field in required_fields}\n\n        try:\n            clean_text = text.replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n            match = re.search(r'\\{.*?\\}', clean_text, re.DOTALL)\n            if match:\n                parsed_json = json.loads(match.group(0))\n                # Guarantee all requested fields exist\n                for key in default_result.keys():\n                    if key not in parsed_json:\n                        parsed_json[key] = \"\"\n                return parsed_json\n        except json.JSONDecodeError:\n            pass\n        return default_result\n\n\n\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-27T12:49:05.895573Z","iopub.execute_input":"2025-09-27T12:49:05.895831Z","iopub.status.idle":"2025-09-27T12:49:05.905757Z","shell.execute_reply.started":"2025-09-27T12:49:05.895811Z","shell.execute_reply":"2025-09-27T12:49:05.904984Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"! pip install pyngrok","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-27T12:49:08.863476Z","iopub.execute_input":"2025-09-27T12:49:08.863727Z","iopub.status.idle":"2025-09-27T12:49:11.929096Z","shell.execute_reply.started":"2025-09-27T12:49:08.863710Z","shell.execute_reply":"2025-09-27T12:49:11.928132Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: pyngrok in /usr/local/lib/python3.11/dist-packages (7.4.0)\nRequirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"!ngrok authtoken 33CVblnubvJ0XkO6OFIoahA1ayu_9wXpjRCRthtwcvgUPQVq","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-27T12:49:12.929064Z","iopub.execute_input":"2025-09-27T12:49:12.929710Z","iopub.status.idle":"2025-09-27T12:49:13.257588Z","shell.execute_reply.started":"2025-09-27T12:49:12.929681Z","shell.execute_reply":"2025-09-27T12:49:13.256637Z"}},"outputs":[{"name":"stdout","text":"Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"import json\nfrom fastapi import FastAPI, HTTPException\nfrom pydantic import BaseModel\n  # Assuming your class is in qwen_mapper.py\nfrom pyngrok import ngrok\n\n# ----------------- FastAPI Setup -----------------\napp = FastAPI(title=\"OCR Field Extractor API\")\n\n# ----------------- Request Body Model -----------------\nclass OCRRequest(BaseModel):\n    text: str\n    fields: list[str]\n\n# ----------------- Initialize the Qwen Model -----------------\nprint(\"Loading model, this may take a few minutes...\")\nmapper = QwenFieldMapper()\nprint(\"Model ready!\")\n\n# ----------------- API Endpoint -----------------\n@app.post(\"/extract\")\ndef extract_fields(request: OCRRequest):\n    try:\n        result = mapper.extract_fields(request.text,request.fields)\n        return result\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=str(e))\nimport nest_asyncio\nnest_asyncio.apply()\n\n# ----------------- Run with ngrok -----------------\nif __name__ == \"__main__\":\n    # Expose API via ngrok\n    public_url = ngrok.connect(8000)\n    print(f\"ngrok tunnel URL: {public_url}\")\n\n    # Start FastAPI server\n    import uvicorn\n    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-27T12:49:16.208603Z","iopub.execute_input":"2025-09-27T12:49:16.208900Z"}},"outputs":[{"name":"stdout","text":"Loading model, this may take a few minutes...\nLoading Qwen model: Qwen/Qwen2.5-1.5B-Instruct\nModel loaded on cuda\nModel ready!\nngrok tunnel URL: NgrokTunnel: \"https://apologal-flutelike-bert.ngrok-free.dev\" -> \"http://localhost:8000\"\n","output_type":"stream"},{"name":"stderr","text":"INFO:     Started server process [36]\nINFO:     Waiting for application startup.\nINFO:     Application startup complete.\nINFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n","output_type":"stream"},{"name":"stdout","text":"INFO:     119.161.98.68:0 - \"POST /extract HTTP/1.1\" 200 OK\n","output_type":"stream"},{"name":"stderr","text":"The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n","output_type":"stream"},{"name":"stdout","text":"INFO:     119.161.98.68:0 - \"POST /extract HTTP/1.1\" 200 OK\n","output_type":"stream"},{"name":"stderr","text":"The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n","output_type":"stream"},{"name":"stdout","text":"INFO:     119.161.98.68:0 - \"POST /extract HTTP/1.1\" 200 OK\n","output_type":"stream"},{"name":"stderr","text":"The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n","output_type":"stream"},{"name":"stdout","text":"INFO:     119.161.98.68:0 - \"POST /extract HTTP/1.1\" 200 OK\n","output_type":"stream"},{"name":"stderr","text":"The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n","output_type":"stream"},{"name":"stdout","text":"INFO:     119.161.98.68:0 - \"POST /extract HTTP/1.1\" 200 OK\n","output_type":"stream"},{"name":"stderr","text":"The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n","output_type":"stream"},{"name":"stdout","text":"INFO:     119.161.98.68:0 - \"POST /extract HTTP/1.1\" 200 OK\n","output_type":"stream"},{"name":"stderr","text":"The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n","output_type":"stream"},{"name":"stdout","text":"INFO:     119.161.98.68:0 - \"POST /extract HTTP/1.1\" 200 OK\n","output_type":"stream"}],"execution_count":null}]}